{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press the corresponding key ('A'-'Z', '.') for the sign or 'q' to quit.\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "directory = 'Otherdata/' \n",
    "\n",
    "if not os.path.exists(directory): \n",
    "    os.mkdir(directory)  \n",
    "\n",
    "\n",
    "for char in ['blank'] + [chr(i) for i in range(65, 91)]: \n",
    "    char_dir = os.path.join(directory, char)  \n",
    "    if not os.path.exists(char_dir): \n",
    "        os.mkdir(char_dir)  \n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)  \n",
    "\n",
    "print(\"Press the corresponding key ('A'-'Z', '.') for the sign or 'q' to quit.\") \n",
    "5\n",
    "while True:\n",
    "   \n",
    "    _, frame = cap.read()  \n",
    "    if not _:  \n",
    "        print(\"Failed to capture frame. Exiting...\") \n",
    "        break \n",
    "\n",
    "    \n",
    "    cv2.rectangle(frame, (0, 40), (300, 300), (255, 255, 255), 2) \n",
    "    cv2.imshow(\"Data Collection\", frame) \n",
    "\n",
    "    \n",
    "    roi = frame[40:300, 0:300]  # Croping the frame (height: 40-300, width: 0-300)\n",
    "    roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) \n",
    "    roi_resized = cv2.resize(roi_gray, (128, 128))  \n",
    "\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xFF  # Wait for a key press.\n",
    "\n",
    "   \n",
    "    if key == ord('q'): \n",
    "        print(\"Exiting...\") \n",
    "        break  \n",
    "\n",
    "    # Capture and save the image if a valid key is pressed\n",
    "    char = chr(key).upper() if key != ord('.') else 'blank' \n",
    "   \n",
    "    if char in [chr(i) for i in range(65, 91)] + ['blank'] + ['Yoo']:\n",
    "        char_dir = os.path.join(directory, char) \n",
    "        image_count = len(os.listdir(char_dir))  \n",
    "        image_path = os.path.join(char_dir, f\"{image_count}.jpg\") \n",
    "        cv2.imwrite(image_path, roi_resized) \n",
    "        print(f\"Image saved: {image_path}\")  \n",
    "\n",
    "\n",
    "cap.release()  \n",
    "cv2.destroyAllWindows() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "132/132 [==============================] - 10s 65ms/step - loss: 3.2930 - accuracy: 0.0479 - val_loss: 3.2920 - val_accuracy: 0.0601\n",
      "Epoch 2/150\n",
      "132/132 [==============================] - 8s 58ms/step - loss: 3.0233 - accuracy: 0.1357 - val_loss: 2.6326 - val_accuracy: 0.2836\n",
      "Epoch 3/150\n",
      "132/132 [==============================] - 8s 58ms/step - loss: 2.1396 - accuracy: 0.3630 - val_loss: 1.4805 - val_accuracy: 0.5640\n",
      "Epoch 4/150\n",
      "132/132 [==============================] - 8s 58ms/step - loss: 1.5200 - accuracy: 0.5204 - val_loss: 1.1446 - val_accuracy: 0.6563\n",
      "Epoch 5/150\n",
      "132/132 [==============================] - 8s 58ms/step - loss: 1.1842 - accuracy: 0.6165 - val_loss: 0.9290 - val_accuracy: 0.7086\n",
      "Epoch 6/150\n",
      "132/132 [==============================] - 8s 58ms/step - loss: 0.9848 - accuracy: 0.6718 - val_loss: 0.7128 - val_accuracy: 0.7998\n",
      "Epoch 7/150\n",
      "132/132 [==============================] - 8s 58ms/step - loss: 0.8540 - accuracy: 0.7129 - val_loss: 0.6869 - val_accuracy: 0.7820\n",
      "Epoch 8/150\n",
      "132/132 [==============================] - 8s 58ms/step - loss: 0.7275 - accuracy: 0.7575 - val_loss: 0.6239 - val_accuracy: 0.7942\n",
      "Epoch 9/150\n",
      "132/132 [==============================] - 8s 57ms/step - loss: 0.6722 - accuracy: 0.7718 - val_loss: 0.5490 - val_accuracy: 0.8387\n",
      "Epoch 10/150\n",
      "132/132 [==============================] - 8s 58ms/step - loss: 0.6034 - accuracy: 0.7923 - val_loss: 0.5400 - val_accuracy: 0.8387\n",
      "Epoch 11/150\n",
      "132/132 [==============================] - 8s 58ms/step - loss: 0.5262 - accuracy: 0.8257 - val_loss: 0.4631 - val_accuracy: 0.8654\n",
      "Epoch 12/150\n",
      "132/132 [==============================] - 8s 57ms/step - loss: 0.4938 - accuracy: 0.8335 - val_loss: 0.4506 - val_accuracy: 0.8610\n",
      "Epoch 13/150\n",
      "132/132 [==============================] - 8s 57ms/step - loss: 0.4555 - accuracy: 0.8455 - val_loss: 0.4532 - val_accuracy: 0.8687\n",
      "Epoch 14/150\n",
      "132/132 [==============================] - 8s 58ms/step - loss: 0.4486 - accuracy: 0.8438 - val_loss: 0.4197 - val_accuracy: 0.8699\n",
      "Epoch 15/150\n",
      "132/132 [==============================] - 8s 57ms/step - loss: 0.4138 - accuracy: 0.8555 - val_loss: 0.3918 - val_accuracy: 0.8910\n",
      "Epoch 16/150\n",
      "132/132 [==============================] - 8s 57ms/step - loss: 0.3537 - accuracy: 0.8834 - val_loss: 0.3635 - val_accuracy: 0.8954\n",
      "Epoch 17/150\n",
      "132/132 [==============================] - 8s 57ms/step - loss: 0.3182 - accuracy: 0.8850 - val_loss: 0.3679 - val_accuracy: 0.8954\n",
      "Epoch 18/150\n",
      "132/132 [==============================] - 8s 57ms/step - loss: 0.3204 - accuracy: 0.8889 - val_loss: 0.4042 - val_accuracy: 0.8966\n",
      "Epoch 19/150\n",
      "132/132 [==============================] - 8s 57ms/step - loss: 0.3572 - accuracy: 0.8824 - val_loss: 0.3624 - val_accuracy: 0.9032\n",
      "Epoch 20/150\n",
      "132/132 [==============================] - 8s 57ms/step - loss: 0.2826 - accuracy: 0.9032 - val_loss: 0.3375 - val_accuracy: 0.8999\n",
      "Epoch 21/150\n",
      "132/132 [==============================] - 8s 57ms/step - loss: 0.2945 - accuracy: 0.8970 - val_loss: 0.3361 - val_accuracy: 0.9121\n",
      "Epoch 22/150\n",
      "132/132 [==============================] - 8s 57ms/step - loss: 0.2556 - accuracy: 0.9158 - val_loss: 0.3398 - val_accuracy: 0.9055\n",
      "Epoch 23/150\n",
      "132/132 [==============================] - 8s 57ms/step - loss: 0.2354 - accuracy: 0.9196 - val_loss: 0.3572 - val_accuracy: 0.9099\n",
      "Epoch 24/150\n",
      "132/132 [==============================] - 8s 57ms/step - loss: 0.2596 - accuracy: 0.9108 - val_loss: 0.3328 - val_accuracy: 0.9088\n",
      "Epoch 25/150\n",
      "132/132 [==============================] - 8s 57ms/step - loss: 0.2386 - accuracy: 0.9172 - val_loss: 0.3036 - val_accuracy: 0.9155\n",
      "Epoch 26/150\n",
      "132/132 [==============================] - 8s 57ms/step - loss: 0.2175 - accuracy: 0.9275 - val_loss: 0.3113 - val_accuracy: 0.9088\n",
      "Epoch 27/150\n",
      "132/132 [==============================] - 7s 56ms/step - loss: 0.2355 - accuracy: 0.9225 - val_loss: 0.3191 - val_accuracy: 0.9132\n",
      "Epoch 28/150\n",
      "132/132 [==============================] - 7s 57ms/step - loss: 0.2289 - accuracy: 0.9237 - val_loss: 0.3312 - val_accuracy: 0.9121\n",
      "Epoch 29/150\n",
      "132/132 [==============================] - 8s 59ms/step - loss: 0.2503 - accuracy: 0.9134 - val_loss: 0.3714 - val_accuracy: 0.9032\n",
      "Epoch 30/150\n",
      "132/132 [==============================] - 8s 59ms/step - loss: 0.1810 - accuracy: 0.9397 - val_loss: 0.3294 - val_accuracy: 0.9221\n",
      "Model training completed and saved.\n",
      "29/29 [==============================] - 1s 16ms/step - loss: 0.2680 - accuracy: 0.9077\n",
      "Test Loss: 0.2680, Test Accuracy: 0.9077\n"
     ]
    }
   ],
   "source": [
    "#Training Part\n",
    "\n",
    "directory = 'Otherdata/'\n",
    "\n",
    "\n",
    "labels = ['blank'] + [chr(i) for i in range(65, 91)]\n",
    "\n",
    "# Initialize lists to hold image data and corresponding labels\n",
    "data = []\n",
    "labels_data = []\n",
    "\n",
    "# Load images from the directories and preprocess\n",
    "for label in labels:\n",
    "    label_dir = os.path.join(directory, label)\n",
    "    \n",
    "    if os.path.exists(label_dir):\n",
    "        for img_name in os.listdir(label_dir):\n",
    "            img_path = os.path.join(label_dir, img_name)\n",
    "            \n",
    "            # Load the image, convert to grayscale, and resize it\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (128, 128))  # Resize to match model input\n",
    "            img = img.astype('float32') / 255.0  # Normalize the pixel values\n",
    "            \n",
    "            # Append image data and its label\n",
    "            data.append(img)\n",
    "            labels_data.append(labels.index(label))  # Numeric label for classification\n",
    "\n",
    "# Convert the data and labels to numpy arrays\n",
    "data = np.array(data)\n",
    "labels_data = np.array(labels_data)\n",
    "\n",
    "# Reshape data to match model input: (samples, height, width, channels)\n",
    "data = data.reshape((data.shape[0], 128, 128, 1))\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data, labels_data, test_size=0.7, random_state=1)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.3, random_state=2)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional Layers with Max Pooling and Dropout for regularization\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Flatten and Fully Connected Layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(labels), activation='softmax'))  # Output layer with softmax activation\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, \n",
    "          epochs=150, \n",
    "          batch_size=32, \n",
    "          validation_data=(X_val, y_val), \n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "# Save the model architecture to a JSON file\n",
    "model_json = model.to_json()\n",
    "with open(\"sign_language_modelnew.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save the model weights to an H5 file\n",
    "model.save_weights(\"sign_language_modelnew.h5\")\n",
    "\n",
    "print(\"Model training completed and saved.\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Press 'q' to exit.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load model architecture\n",
    "json_file_path = \"sign_language_modelnew.json\"  # Path to the .json file\n",
    "weights_file_path = \"sign_language_modelnew.h5\"  # Path to the .h5 weights file\n",
    "\n",
    "# Load JSON and create model\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    model_json = json_file.read()\n",
    "model = model_from_json(model_json)\n",
    "\n",
    "# Load weights into the model\n",
    "model.load_weights(weights_file_path)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image):\n",
    "    feature = np.array(image, dtype=\"float32\")\n",
    "    feature = feature.reshape(1, 128, 128, 1)  # Ensure the input shape matches the model\n",
    "    return feature / 255.0  # Normalize pixel values\n",
    "\n",
    "# Labels for classification\n",
    "label = ['blank'] + [chr(i) for i in range(65, 91)]  # 'blank' + 'A'-'Z'\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 'q' to exit.\")\n",
    "\n",
    "while True:\n",
    "    # Read frame from webcam\n",
    "    _, frame = cap.read()\n",
    "    if not _:\n",
    "        print(\"Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Draw rectangle for ROI\n",
    "    cv2.rectangle(frame, (0, 40), (300, 300), (0, 165, 255), 2)\n",
    "\n",
    "    # Crop and preprocess the ROI\n",
    "    roi = frame[40:300, 0:300]\n",
    "    roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    roi_resized = cv2.resize(roi_gray, (128, 128))\n",
    "    processed_roi = preprocess_image(roi_resized)\n",
    "\n",
    "    # Make prediction\n",
    "    pred = model.predict(processed_roi, verbose=0)\n",
    "    prediction_label = label[pred.argmax()]\n",
    "    accuracy = np.max(pred) * 100\n",
    "\n",
    "    # Display prediction\n",
    "    display_text = f\"{prediction_label} ({accuracy:.2f}%)\" if prediction_label != 'blank' else \"No Sign\"\n",
    "    cv2.rectangle(frame, (0, 0), (300, 40), (0, 165, 255), -1)\n",
    "    cv2.putText(frame, display_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the output\n",
    "    cv2.imshow(\"Sign Language Recognition\", frame)\n",
    "\n",
    "    # Exit on 'q' key\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
